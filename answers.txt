PartOne (d) Flesch-Kincaid reading scores based mainly on sentence length and syllable count don’t always show how difficult 
a text really is. For example, one situation where they can be unreliable is when a text uses complex or 
rare vocabulary. For example, in my analysis, some novels with relatively low scores still contained difficult words or ideas.
This shows that these measures might underestimate difficulty if they only look at sentence and word length but ignore
vocabulary complexity.

Another case is when sentence length and structure don’t reflect actual comprehension difficulty.
Short sentences can still be confusing if the text is unclear or lacks coherence.Also, longer words aren’t always harder,
and some short words might be tricky. Overall, these scores give a rough estimate 
of readability but should be combined with other measures or human judgment to fully understand text difficulty.


PartTwo (f): 
For my custom tokenizer, I used NLTK’s word_tokenize to split the text into individual words.
I filtered the tokens by keeping only alphabetic words and converted everything to lowercase, so that words like "Prime" 
and "prime" are counted the same. I also removed English stopwords using NLTK’s built-in stopword list to reduce the 
number of useless features from common words like "the" or "and", which don’t help with classification.

The aim was to make the feature set smaller and more focused on words that actually help tell the political parties apart.
I used this tokenizer with TfidfVectorizer and kept the feature limit at 3000, the same as the other parts of the coursework,
so the comparison is fair.

In terms of results, my tokenizer gave a Random Forest macro F1 score of 0.4489, and the SVM got 0.5742.
These results were similar to the default n-gram setup, but the custom tokenizer helped by reducing noise in the features.
It didn’t boost the F1 score massively, but it simplified the feature space and made the model slightly more efficient.
Further improvements would probably need a more advanced tokenizer or different features.

